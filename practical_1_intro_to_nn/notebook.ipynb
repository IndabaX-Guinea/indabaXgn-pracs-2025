{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical 1: Introduction to Neural Networks\n",
    "\n",
    "Welcome to the first practical session! In this notebook, you will learn the fundamentals of neural networks.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup and Imports](#setup)\n",
    "2. [Understanding Neurons](#neurons)\n",
    "3. [Building a Simple Neural Network](#building)\n",
    "4. [Training on MNIST](#mnist)\n",
    "5. [Evaluation and Experimentation](#evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports <a id='setup'></a>\n",
    "\n",
    "First, let's import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils.helper_functions import set_seed, plot_training_history, get_device, print_model_summary\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "set_seed(42)\n",
    "\n",
    "# Get device\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding Neurons <a id='neurons'></a>\n",
    "\n",
    "### Exercise 1.1: Implement a Single Neuron\n",
    "\n",
    "A neuron performs a weighted sum of inputs and applies an activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid activation function\"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def single_neuron(inputs, weights, bias):\n",
    "    \"\"\"\n",
    "    Implement a single neuron with sigmoid activation.\n",
    "    \n",
    "    Args:\n",
    "        inputs: Input array\n",
    "        weights: Weight array\n",
    "        bias: Bias term\n",
    "    \n",
    "    Returns:\n",
    "        Output after sigmoid activation\n",
    "    \"\"\"\n",
    "    # TODO: Implement the neuron\n",
    "    # 1. Compute weighted sum: z = sum(inputs * weights) + bias\n",
    "    # 2. Apply sigmoid activation\n",
    "    pass\n",
    "\n",
    "# Test your implementation\n",
    "test_inputs = np.array([0.5, 0.3, 0.2])\n",
    "test_weights = np.array([0.4, 0.7, 0.2])\n",
    "test_bias = 0.1\n",
    "\n",
    "output = single_neuron(test_inputs, test_weights, test_bias)\n",
    "print(f\"Neuron output: {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building a Simple Neural Network <a id='building'></a>\n",
    "\n",
    "### Exercise 1.2: Define a Multi-Layer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        # TODO: Define layers\n",
    "        # Hint: Use nn.Linear for fully connected layers\n",
    "        # You need: input -> hidden -> output\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # TODO: Implement forward pass\n",
    "        # Hint: Don't forget activation functions (ReLU)\n",
    "        pass\n",
    "\n",
    "# Create a model instance\n",
    "model = SimpleMLP(input_size=784, hidden_size=128, output_size=10)\n",
    "print_model_summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training on MNIST <a id='mnist'></a>\n",
    "\n",
    "### Load the MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = datasets.MNIST('../data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST('../data', train=False, transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Some Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of training data\n",
    "examples = iter(train_loader)\n",
    "example_data, example_targets = next(examples)\n",
    "\n",
    "# Plot some examples\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(example_data[i][0], cmap='gray')\n",
    "    ax.set_title(f'Label: {example_targets[i]}')\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.3: Implement Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"\n",
    "    Train for one epoch.\n",
    "    \n",
    "    Returns:\n",
    "        Average loss and accuracy for the epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # TODO: Implement training step\n",
    "        # 1. Move data to device\n",
    "        # 2. Flatten the images (28x28 -> 784)\n",
    "        # 3. Zero gradients\n",
    "        # 4. Forward pass\n",
    "        # 5. Compute loss\n",
    "        # 6. Backward pass\n",
    "        # 7. Update weights\n",
    "        # 8. Track loss and accuracy\n",
    "        pass\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = 100. * correct / len(train_loader.dataset)\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def evaluate(model, test_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Evaluate the model.\n",
    "    \n",
    "    Returns:\n",
    "        Average loss and accuracy\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            # TODO: Implement evaluation\n",
    "            # Similar to training but without gradient computation\n",
    "            pass\n",
    "    \n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, loss, and optimizer\n",
    "model = SimpleMLP(input_size=784, hidden_size=128, output_size=10).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "train_losses, train_accs = [], []\n",
    "val_losses, val_accs = [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc = evaluate(model, test_loader, criterion, device)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}: '\n",
    "          f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% | '\n",
    "          f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(train_losses, val_losses, train_accs, val_accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation and Experimentation <a id='evaluation'></a>\n",
    "\n",
    "### Exercise 1.4: Experiment with Different Architectures\n",
    "\n",
    "Try modifying:\n",
    "- Number of hidden layers\n",
    "- Hidden layer size\n",
    "- Activation functions\n",
    "- Learning rate\n",
    "- Batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Experiment with different configurations\n",
    "# Example: Try a deeper network with 2 hidden layers\n",
    "\n",
    "class DeeperMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        super(DeeperMLP, self).__init__()\n",
    "        # TODO: Implement a deeper architecture\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # TODO: Implement forward pass\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations! ðŸŽ‰\n",
    "\n",
    "You've completed Practical 1! You should now understand:\n",
    "- How neurons and neural networks work\n",
    "- How to implement forward and backward propagation\n",
    "- How to train a neural network on real data\n",
    "- How different hyperparameters affect training\n",
    "\n",
    "### Next Steps\n",
    "- Try different activation functions\n",
    "- Implement dropout for regularization\n",
    "- Add batch normalization\n",
    "- Move on to Practical 2: CNNs!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
